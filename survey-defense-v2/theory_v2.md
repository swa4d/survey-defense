### Part 1: The Four Base Classification Techniques
Before making a final decision, the system evaluates the survey respondent using four distinct classification techniques or "layers". 

**1. Heuristic Risk Engine (Rule-Based Classification)**
This layer calculates interpretable risk subscores bounded between 0 and 1 by heavily weighting known bot behaviors. It evaluates three domains:
* **Automation Risk:** Checks for headless browsers (like PhantomJS or WebDriver).
* **Behavioral Risk:** Penalizes unnatural speed (e.g., completing pages in under 2 seconds) and excessive copy/pasting. *Equation (Sigmoid Penalty for Paste Ratio)*: $Risk = \frac{1}{1 + e^{-10 \cdot (PasteRatio - 0.6)}}$
* **Duplication Risk:** Looks for platform-level deduplication flags or "ballot box stuffing," setting a hard risk floor of 0.80 if a duplicate is found.

**2. The Pseudo-Labeler (Weak Supervision)**
Because survey data lacks true "ground truth" labels, this logic acts as a deterministic classifier to create them. It pits "bot signals" (e.g., 35.x IP subnets, fast clicking) against "human signals" (mid-range duration, no duplication). 
* **Logic:** If $BotSignals \ge 3$ and outnumber human signals, the response is labeled a Bot ($1$). If $HumanSignals \ge 6$ and $BotSignals \le 1$, it is a Human ($-1$).


**3. Unsupervised Anomaly Detector (Isolation Forest)**
This algorithm assumes human behavior generally clusters together, while bots behave as statistical outliers. It measures the distance of a respondent's behavioral and network features from the norm. 
* *Equation (Score Conversion)*: The raw decision distance is converted into a 0â€“1 probability using the expit (inverse logit) function: $P(Anomaly) = \frac{1}{1 + e^{5 \cdot RawScore}}$.

**4. Supervised Classifier (Gradient Boosting)**
Using the high-confidence labels generated by the Pseudo-Labeler, this layer trains a LightGBM or Gradient Boosting decision tree. It dynamically adjusts its depth and complexity based on the number of pseudo-labeled bots available.

---

### Part 2: The Meta-Classifier (The Ensemble Vote)

The final layer decides the ultimate "bot probability" by acting as a judge that weighs the outputs of the previous classifiers. 

**Log-Odds Transformation**
Because the inputs are bounded probabilities (0 to 1), feeding them directly into a linear model causes mathematical issues. The ensemble uses the **logit equation** to transform the scores into an unbounded space. To prevent infinite values, scores are clipped between $10^{-6}$ and $1 - 10^{-6}$:
$$logit(p) = \ln\left(\frac{p}{1-p}\right)$$

**Logistic Regression & Calibration**
The transformed scores are fed into a Logistic Regression model trained via Cross-Validation (`CalibratedClassifierCV`). This ensures the final output is a true probability curve rather than just an arbitrary score.

**The Fallback Equation**
If the dataset has too few definitive bots (fewer than 5) to train the meta-classifier, the ensemble falls back to a hardcoded weighted vote:
$$P(Bot) = 0.40(DupRisk) + 0.30(BehavRisk) + 0.15(AutoRisk) + 0.15(AnomalyRisk)$$

---

### Part 3: The Data Pipeline (Feature Engineering)
Before any classification occurs, the raw survey data is normalized and transformed into a mathematical matrix. 

* **Timing Standardization:** It converts raw durations into Z-scores to measure how far a respondent deviates from the median time.
* **Uniformity Checks:** Bots click at highly consistent intervals. The pipeline checks the coefficient of variation (CV) for page submission times: 
    $$CV = \frac{\sigma_{submit}}{\mu_{submit}}$$ 
* **Click Entropy:** It calculates the normalized Shannon entropy of click counts to see if mouse behavior is natural (random) or robotic (highly predictable):
    $$H = -\frac{\sum p_i \log_2(p_i)}{\log_2(N)}$$
* **Network Fingerprinting:** Extracts the first three octets of the IP address (the /24 subnet) to count how many responses came from the same server farm.